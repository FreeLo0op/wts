#!/usr/bin/env python3
import os
import argparse
import json
from pathlib import Path
from safetensors import safe_open
import torch
import humanize
from collections import defaultdict

def get_file_size(file_path):
    """è·å–æ–‡ä»¶å¤§å°ï¼ˆäººç±»å¯è¯»æ ¼å¼ï¼‰"""
    return humanize.naturalsize(os.path.getsize(file_path))

def categorize_layer(layer_name):
    """æ ¹æ®å±‚åç§°åˆ†ç±»"""
    categories = {
        'whisper_model': 'whisper_model',
        'model.layers': 'model.layers', 
        'model.mimo_layers': 'model.mimo_layers',
        'lm_head': 'lm_head',
        'mimo_output': 'mimo_output'
    }
    
    for category_prefix in categories:
        if layer_name.startswith(category_prefix):
            return categories[category_prefix]
    
    return 'other'

def calculate_layer_sizes(safetensors_path):
    """è®¡ç®—safetensorsæ–‡ä»¶ä¸­æ¯å±‚çš„å‚æ•°å¤§å°"""
    layer_sizes = {}
    total_size_bytes = 0
    
    try:
        with safe_open(safetensors_path, framework="pt", device="cpu") as f:
            for key in f.keys():
                tensor = f.get_tensor(key)
                # è®¡ç®—å‚æ•°æ•°é‡
                num_params = tensor.numel()
                # è®¡ç®—å†…å­˜å¤§å°ï¼ˆå­—èŠ‚ï¼‰
                size_bytes = tensor.numel() * tensor.element_size()
                total_size_bytes += size_bytes
                
                layer_sizes[key] = {
                    'shape': list(tensor.shape),
                    'num_params': num_params,
                    'size_bytes': size_bytes,
                    'size_human': humanize.naturalsize(size_bytes),
                    'dtype': str(tensor.dtype),
                    'category': categorize_layer(key)
                }
                
    except Exception as e:
        print(f"Error reading {safetensors_path}: {e}")
        return None, 0
    
    return layer_sizes, total_size_bytes

def analyze_safetensors_directory(directory):
    """åˆ†æç›®å½•ä¸‹çš„æ‰€æœ‰safetensorsæ–‡ä»¶"""
    directory = Path(directory)
    
    if not directory.exists():
        print(f"Error: Directory {directory} does not exist")
        return
    
    safetensors_files = list(directory.rglob("*.safetensors"))
    
    if not safetensors_files:
        print(f"No .safetensors files found in {directory}")
        return
    
    print(f"Found {len(safetensors_files)} .safetensors files in {directory}")
    print("=" * 80)
    
    # åˆå§‹åŒ–åˆ†ç±»ç»Ÿè®¡
    category_stats = defaultdict(lambda: {
        'total_params': 0,
        'total_size_bytes': 0,
        'layer_count': 0,
        'layers': []
    })
    
    total_files_size = 0
    all_layers = {}
    
    for file_path in safetensors_files:
        print(f"\nğŸ“ Analyzing: {file_path.relative_to(directory)}")
        print(f"ğŸ“ File size: {get_file_size(file_path)}")
        
        layer_sizes, total_size_bytes = calculate_layer_sizes(file_path)
        
        if layer_sizes is None:
            continue
            
        total_files_size += total_size_bytes
        all_layers.update(layer_sizes)
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        for layer_name, info in layer_sizes.items():
            category = info['category']
            category_stats[category]['total_params'] += info['num_params']
            category_stats[category]['total_size_bytes'] += info['size_bytes']
            category_stats[category]['layer_count'] += 1
            category_stats[category]['layers'].append(layer_name)
    
    # æ‰“å°åˆ†ç±»ç»Ÿè®¡
    print_category_summary(category_stats, total_files_size)
    
    # æ‰“å°è¯¦ç»†å±‚ä¿¡æ¯
    print_detailed_layer_info(all_layers)
    
    return all_layers, category_stats

def print_category_summary(category_stats, total_size_bytes):
    """æ‰“å°æŒ‰ç±»åˆ«åˆ†ç±»çš„ç»Ÿè®¡ä¿¡æ¯"""
    print("\n" + "=" * 80)
    print("ğŸ¯ CATEGORY SUMMARY")
    print("=" * 80)
    
    # å®šä¹‰ç±»åˆ«æ˜¾ç¤ºé¡ºåº
    category_order = [
        'whisper_model',
        'model.layers', 
        'model.mimo_layers',
        'lm_head',
        'mimo_output',
        'other'
    ]
    
    print(f"\n{'Category':<20} {'Layers':<8} {'Params':<15} {'Size':<15} {'% of Total':<10}")
    print("-" * 80)
    
    total_params = sum(stats['total_params'] for stats in category_stats.values())
    
    for category in category_order:
        if category in category_stats:
            stats = category_stats[category]
            param_pct = (stats['total_params'] / total_params) * 100 if total_params > 0 else 0
            size_pct = (stats['total_size_bytes'] / total_size_bytes) * 100 if total_size_bytes > 0 else 0
            
            print(f"{category:<20} {stats['layer_count']:<8} "
                  f"{humanize.intcomma(stats['total_params']):<15} "
                  f"{humanize.naturalsize(stats['total_size_bytes']):<15} "
                  f"{size_pct:.1f}%")

def print_detailed_layer_info(all_layers):
    """æ‰“å°æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†å±‚ä¿¡æ¯"""
    print("\n" + "=" * 80)
    print("ğŸ“‹ DETAILED LAYER INFORMATION")
    print("=" * 80)
    
    # æŒ‰ç±»åˆ«åˆ†ç»„
    categorized_layers = defaultdict(dict)
    for layer_name, info in all_layers.items():
        categorized_layers[info['category']][layer_name] = info
    
    # æ‰“å°æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†ä¿¡æ¯
    for category in ['whisper_model', 'model.layers', 'model.mimo_layers', 'lm_head', 'mimo_output', 'other']:
        if category in categorized_layers and categorized_layers[category]:
            print(f"\nğŸ·ï¸  {category.upper()} Layers:")
            print("-" * 100)
            print(f"{'Layer Name':<60} {'Shape':<20} {'Params':<15} {'Size':<15}")
            print("-" * 100)
            
            # æŒ‰å¤§å°æ’åº
            sorted_layers = sorted(categorized_layers[category].items(), 
                                 key=lambda x: x[1]['size_bytes'], 
                                 reverse=True)
            
            for layer_name, info in sorted_layers:
                params_str = humanize.intcomma(info['num_params'])
                print(f"{layer_name:<60} {str(info['shape']):<20} {params_str:<15} {info['size_human']:<15}")

def save_to_json(all_layers, category_stats, output_path):
    """å°†ç»“æœä¿å­˜ä¸ºJSONæ–‡ä»¶"""
    output_data = {
        'total_parameters': sum(l['num_params'] for l in all_layers.values()),
        'total_size_bytes': sum(l['size_bytes'] for l in all_layers.values()),
        'category_stats': {},
        'layers': all_layers
    }
    
    for category, stats in category_stats.items():
        output_data['category_stats'][category] = {
            'total_params': stats['total_params'],
            'total_size_bytes': stats['total_size_bytes'],
            'layer_count': stats['layer_count'],
            'layers': stats['layers']
        }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ Results saved to: {output_path}")

def print_overall_statistics(all_layers, category_stats):
    """æ‰“å°æ€»ä½“ç»Ÿè®¡ä¿¡æ¯"""
    total_params = sum(l['num_params'] for l in all_layers.values())
    total_size = sum(l['size_bytes'] for l in all_layers.values())
    
    print("\n" + "=" * 80)
    print("ğŸ“Š OVERALL STATISTICS")
    print("=" * 80)
    print(f"Total parameters: {humanize.intcomma(total_params)}")
    print(f"Total model size: {humanize.naturalsize(total_size)}")
    print(f"Total layers: {len(all_layers)}")
    
    # æ‰“å°æ¯ä¸ªç±»åˆ«çš„å‚æ•°åˆ†å¸ƒ
    print(f"\nParameter distribution by category:")
    for category, stats in category_stats.items():
        pct = (stats['total_params'] / total_params) * 100
        print(f"  {category}: {pct:.1f}%")

def main():
    parser = argparse.ArgumentParser(description='Analyze .safetensors files and calculate layer sizes by category')
    parser.add_argument('directory', type=str, help='Directory to search for .safetensors files')
    parser.add_argument('--output', '-o', type=str, help='Output JSON file path')
    parser.add_argument('--verbose', '-v', action='store_true', help='Verbose output')
    
    args = parser.parse_args()
    
    print(f"ğŸ” Searching for .safetensors files in: {args.directory}")
    
    all_layers, category_stats = analyze_safetensors_directory(args.directory)
    
    # æ‰“å°æ€»ä½“ç»Ÿè®¡
    if all_layers:
        print_overall_statistics(all_layers, category_stats)
    
    # ä¿å­˜ç»“æœ
    if args.output and all_layers:
        save_to_json(all_layers, category_stats, args.output)

if __name__ == "__main__":
    main()